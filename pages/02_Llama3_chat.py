import streamlit as st
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema.runnable import RunnablePassthrough
from langchain.callbacks.base import BaseCallbackHandler
from langchain.memory import ConversationTokenBufferMemory
from langchain_community.chat_models import ChatOllama
from langchain.globals import set_llm_cache
from langchain.cache import SQLiteCache

set_llm_cache(SQLiteCache("cache.db"))

st.set_page_config(
    page_title="Llama3",
    page_icon="ğŸ“ƒ",
)

callback = False


class ChatCallbackHandler(BaseCallbackHandler):
    message = ""

    def on_llm_start(self, *args, **kwargs):
        if callback:
            self.message_box = st.empty()

    def on_llm_end(self, *args, **kwargs):
        if callback:
            save_messages(self.message, "ai")

    def on_llm_new_token(self, token, *args, **kwargs):
        if callback:
            self.message += token
            self.message_box.markdown(self.message)


if "llama3_messages" not in st.session_state:
    st.session_state["llama3_messages"] = []


prompt_message_llammachat = """ì„¤ëª…: í•˜ë“œì›¨ì–´ ë° ì†Œí”„íŠ¸ì›¨ì–´ ì „ë¬¸ê°€ë¡œì„œ ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ë¬¸ì˜ë‚˜ ì§„ìˆ ì— ëŒ€í•´ ìƒì„¸í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. 
í•˜ë“œì›¨ì–´ ë° ì†Œí”„íŠ¸ì›¨ì–´ì™€ ê´€ë ¨ëœ ë³µì¡í•œ ê°œë…ì„ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì—¬ ê´‘ë²”ìœ„í•œ ì²­ì¤‘ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í•´ì•¼ í•©ë‹ˆë‹¤. 
ì„¤ëª…ì„ ë‹¨ìˆœí™”í•  ë¿ë§Œ ì•„ë‹ˆë¼ ê°€ëŠ¥í•œ í•œ ê´€ë ¨ ì‚¬ë¡€ë¥¼ ì œì‹œí•˜ì—¬ ì§ˆë¬¸ìì˜ ì´í•´ë„ë¥¼ ë†’ì´ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.

ì—­í• : í•˜ë“œì›¨ì–´ ë° ì†Œí”„íŠ¸ì›¨ì–´ ì „ë¬¸ê°€
ëª©í‘œ: ì§ˆë¬¸ìê°€ í•˜ë“œì›¨ì–´ ë° ì†Œí”„íŠ¸ì›¨ì–´ ê°œë…ì„ í¬ê´„ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ”ë‹¤.

ê°€ì´ë“œë¼ì¸
1. ë³µì¡í•œ ê¸°ìˆ  ì£¼ì œë¥¼ ê°„ë‹¨í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…ìœ¼ë¡œ ì„¸ë¶„í™”í•©ë‹ˆë‹¤.
2. ê¸°ìˆ ì  ë°°ê²½ ì§€ì‹ì´ ì—†ëŠ” ê°œì¸ë„ ì´í•´í•  ìˆ˜ ìˆëŠ” ëª…í™•í•˜ê³  ì ‘ê·¼í•˜ê¸° ì‰¬ìš´ ì–¸ì–´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
3. ì‹¤ì œ ì‚¬ë¡€ë‚˜ ê°€ìƒì˜ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì œì‹œí•˜ì—¬ ì„¤ëª…ì˜ ì‹¤ë¡€ë¥¼ ë³´ì—¬ì£¼ê³  ì¶”ìƒì ì¸ ê°œë…ì„ êµ¬ì²´í™”í•©ë‹ˆë‹¤.
4. í”„ë¡œì„¸ìŠ¤ì™€ ê¸°ìˆ ì˜ 'ë°©ë²•'ê³¼ 'ì´ìœ 'ë¥¼ ì„¤ëª…í•˜ì—¬ ì§ˆë¬¸ìì˜ ì´í•´ë¥¼ ê¹Šê²Œ í•©ë‹ˆë‹¤.
5. ì†Œí”„íŠ¸ì›¨ì–´ì— ëŒ€í•´ ë…¼ì˜í•  ë•ŒëŠ” ì†Œí”„íŠ¸ì›¨ì–´ê°€ í•˜ë“œì›¨ì–´ì™€ ì–´ë–»ê²Œ ìƒí˜¸ì‘ìš©í•˜ì—¬ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•˜ëŠ”ì§€ ì„¤ëª…í•˜ì„¸ìš”.

ì£¼ì œ ì˜ˆì‹œ
- CPUëŠ” ì–´ë–»ê²Œ ëª…ë ¹ì„ ì²˜ë¦¬í•˜ë‚˜ìš”?
- ì»´í“¨í„°ì—ì„œ ìš´ì˜ ì²´ì œì˜ ì—­í• ì€ ë¬´ì—‡ì¸ê°€ìš”?
- ìŠ¤ë§ˆíŠ¸í°ì´ í•˜ë“œì›¨ì–´ì™€ ì†Œí”„íŠ¸ì›¨ì–´ë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ì—¬ ì‚¬ì§„ì„ ìº¡ì²˜í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•  ìˆ˜ ìˆë‚˜ìš”?
- ì½”ë“œë¥¼ ì‹¤í–‰ ê°€ëŠ¥í•œ í”„ë¡œê·¸ë¨ìœ¼ë¡œ ì»´íŒŒì¼í•˜ëŠ” ê³¼ì •ì„ ì„¤ëª…í•  ìˆ˜ ìˆë‚˜ìš”?
"""
# """Explanation: As a hardware and software expert, your task is to provide detailed and easily understandable explanations in response to inquiries or statements. 
# You are expected to demystify complex concepts related to both hardware and software, making them accessible to a broad audience. 
# Your objective is to enhance the questioner's comprehension by not only simplifying explanations but also by providing relevant examples whenever possible.

# Role: Hardware and Software Expert
# Objective: To assist questioners in comprehensively understanding hardware and software concepts

# Guidelines:
# 1. Break down complex technical topics into simple, digestible explanations.
# 2. Use clear and accessible language that can be understood by individuals without a technical background.
# 3. Provide real-life examples or hypothetical scenarios to illustrate your explanations and make abstract concepts tangible.
# 4. Address the 'how' and 'why' behind processes and technologies to deepen the questioner's understanding.
# 5. When discussing software, explain how it interacts with hardware to perform its functions.

# Example Topics:
# - How do CPUs process instructions?
# - What is the role of an operating system in a computer?
# - Can you explain how a smartphone uses both hardware and software to capture and process photos?
# - Describe the process of compiling code into an executable program.

# When answering, it's important to remember that your goal is to make the information as accessible as possible. 
# Strive to not only answer the question but also to educate the questioner, providing them with a foundation that enables them to grasp more complex concepts in the future."""

prompt_translate="Translate the sentence you answered before into Korean."

def set_prompt():
    return prompt_message_llammachat


with st.sidebar:
    prompt_text = st.text_area(
        "Prompt", set_prompt(),
    )


llm = ChatOllama(
    temperature=0.1,
    model="llama3.1:latest",
    streaming=True,
    callbacks=[ChatCallbackHandler()],
)

memory = ConversationTokenBufferMemory(
    llm=llm,
    max_token_limit=2000,
    return_messages=True,
)

if "llama3_chat_summary" not in st.session_state:
    st.session_state["llama3_chat_summary"] = []
    st.session_state["last_answer"] = ""
else:
    callback = False
    if st.session_state["llama3_chat_summary"]:
        st.session_state["last_answer"] = st.session_state["llama3_chat_summary"][-1]["answer"]

    for chat_list in st.session_state["llama3_chat_summary"]:
        memory.save_context(
            {"input": chat_list["question"]},
            {"output": chat_list["answer"]},
        )


def save_messages(message, role):
    st.session_state["llama3_messages"].append(
        {
            "message": message,
            "role": role,
        }
    )


def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_messages(message, role)


def paint_history():
    for message in st.session_state["llama3_messages"]:
        send_message(message["message"], message["role"], save=False)

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            f"""
            {prompt_text}
            """,
        ),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{question}"),
    ]
)

def load_memory(_):
    loaded_memory = memory.load_memory_variables({})["history"]
    return loaded_memory


def save_context(question, result):
    st.session_state["llama3_chat_summary"].append(
        {
            "question": question,
            "answer": result,
        }
    )

def invoke_chain(question):
    result = chain.invoke(
        {"question": question},
    )
    save_context(message, result.content)

@st.spinner(text="translating...")
def translate_answer():
    sentence = st.session_state["last_answer"]
    print(sentence)
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                f"""
                {prompt_translate}
                """,
            ),
            ("human", "{question}"),
        ]
    )
    translate_llm = ChatOllama(
        temperature=0.1,
        model="llama3.1:latest",
        streaming=True,
        callbacks=[ChatCallbackHandler()],
    )
    
    chain = prompt | translate_llm
    chain.invoke(
        {"question": sentence}
    )

st.title("Llama3 Chatbot")

st.markdown(
    """
    Welcome!
    
    Use this chatbot to ask questions to an AI about your Questions!
    
    """
)

send_message("I'm ready! Ask away!", "ai", save=False)
paint_history()

authentication_status = st.session_state["authentication_status"]
if authentication_status:
    message = st.chat_input("Ask anything about something...")

    if message:
        send_message(message, "human")
        chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm

        with st.chat_message("ai"):
            callback = True
            invoke_chain(message)
    
    if st.button("translate"):
        with st.chat_message("ai"):
            callback = True
            translate_answer()

